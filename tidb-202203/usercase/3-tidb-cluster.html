<!DOCTYPE HTML>
<html lang="zh-CN" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>TiDB集群恢复之TiKV集群不可用 - TiDB 社区技术月刊</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../index.html">TiDB 社区技术月刊</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/prefix-feb.html"><strong aria-hidden="true">1.</strong> 2022 年 2 月刊</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202202/failure-analysis/failure-analysis.html"><strong aria-hidden="true">1.1.</strong> 故障解读</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202202/failure-analysis/1-br.html"><strong aria-hidden="true">1.1.1.</strong> v5.3.0 - BR 备份报错并且耗时比升级前更长(v2)</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/failure-analysis/2-ticdc.html"><strong aria-hidden="true">1.1.2.</strong> v5.1.2 - TiCDC 不同步，checkpointTs 不推进</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/failure-analysis/3-tiflash-sql.html"><strong aria-hidden="true">1.1.3.</strong> v5.1.1 - 调整变量 tidb_isolation_read_engines 影响 tiflash SQL 执行计划</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202202/usercase/usercase.html"><strong aria-hidden="true">1.2.</strong> 技术分享</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202202/usercase/1-tikv.html"><strong aria-hidden="true">1.2.1.</strong> 诊断 SOP | TiKV/TiFlash 下线慢</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/usercase/2-sop-gc.html"><strong aria-hidden="true">1.2.2.</strong> 诊断 SOP | GC 相关问题排查</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/usercase/3-tidb-lightning.html"><strong aria-hidden="true">1.2.3.</strong> 最佳实践 | tidb-lightning 使用 tidb-backend 模式导入优化</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/usercase/4-region-cache.html"><strong aria-hidden="true">1.2.4.</strong> 原理解读 | Region Cache 缓存和清理逻辑解释</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202202/product-news/update.html"><strong aria-hidden="true">1.3.</strong> TiDB 产品资讯</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202202/product-news/1-roadmap.html"><strong aria-hidden="true">1.3.1.</strong> 发版计划</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/product-news/2-bug.html"><strong aria-hidden="true">1.3.2.</strong> 严重 Bug 及兼容性问题</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202202/community-news/community-news.html"><strong aria-hidden="true">1.4.</strong> TiDB 社区动态</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202202/community-news/1-mva.html"><strong aria-hidden="true">1.4.1.</strong> 2 月 MOA/MVA</a></li><li class="chapter-item expanded "><a href="../../tidb-202202/community-news/2-tidb-certification.html"><strong aria-hidden="true">1.4.2.</strong> TiDB 能力认证</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202203/prefix-mar.html"><strong aria-hidden="true">2.</strong> 2022 年 3 月刊</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/update.html"><strong aria-hidden="true">2.1.</strong> 产品更新</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/tidb-6.0.html"><strong aria-hidden="true">2.1.1.</strong> TiDB 6.0 发版：向企业级云数据库迈进</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/tiflash.html"><strong aria-hidden="true">2.1.2.</strong> TiFlash 开源了</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/tiflash-resource.html"><strong aria-hidden="true">2.1.3.</strong> TiFlash 快速上手资源汇总</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/bug.html"><strong aria-hidden="true">2.2.</strong> 产品 Bug 及兼容性问题</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/tiflash-crash.html"><strong aria-hidden="true">2.2.1.</strong> TiFlash 在开启了 TLS 的情况下会随机 crash</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/tiflash-v5_4_0.html"><strong aria-hidden="true">2.2.2.</strong> TiFlash 从低版本升级到 v5.4.0 后，元数据丢失，无法响应查询请求</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/tikv-nodes.html"><strong aria-hidden="true">2.2.3.</strong> 由于 Placement Rule 设置不合理导致 TiKV 节点不均衡</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/development.html"><strong aria-hidden="true">2.3.</strong> 开发适配</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/ent-tidb.html"><strong aria-hidden="true">2.3.1.</strong> Facebook 开源 Golang 实体框架 Ent 现已支持 TiDB</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/product-news/flink-tidb.html"><strong aria-hidden="true">2.3.2.</strong> Flink CDC 2.2 正式发布，新增 TiDB 数据源，新增 TiDB CDC 连接器</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/usercase.html"><strong aria-hidden="true">2.4.</strong> 用户实践</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/1-infra-value.html"><strong aria-hidden="true">2.4.1.</strong> 黄东旭： 关于基础软件产品价值的思考</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/2-inspur-tidb.html"><strong aria-hidden="true">2.4.2.</strong> 国产化浪潮下 TiDB 解决的痛点问题</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/3-tidb-cluster.html" class="active"><strong aria-hidden="true">2.4.3.</strong> TiDB集群恢复之TiKV集群不可用</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/4-PointGet.html"><strong aria-hidden="true">2.4.4.</strong> PointGet 的一生</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/5-TiDB-code.html"><strong aria-hidden="true">2.4.5.</strong> TiDB 源码系列之沉浸式编译 TiDB</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/6-ctrip-tidb.html"><strong aria-hidden="true">2.4.6.</strong> 分布式数据库TiDB在携程的实践</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/7-netease-tidb.html"><strong aria-hidden="true">2.4.7.</strong> 网易这么牛的迁移方案你学会了吗？</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/8-tidb-server-oom.html"><strong aria-hidden="true">2.4.8.</strong> TiDB Server 的 OOM 问题优化探索</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/9-ccbfintech-chaos.html"><strong aria-hidden="true">2.4.9.</strong> 混沌工程在建信金科的应用实践</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/10-tuning-hardware.html"><strong aria-hidden="true">2.4.10.</strong> 数据库调优之硬件</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/11-TiDB-HATP.html"><strong aria-hidden="true">2.4.11.</strong> TiDB的HATP对我们来说意味着什么？</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/usercase/12-TiEM-test.html"><strong aria-hidden="true">2.4.12.</strong> TiEM 初体验</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202203/community-news/community-news.html"><strong aria-hidden="true">2.5.</strong> 社区动态</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202203/community-news/event-summary.html"><strong aria-hidden="true">2.5.1.</strong> 近期活动预告</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/community-news/upcoming-event.html"><strong aria-hidden="true">2.5.2.</strong> 本月精彩活动回顾</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/community-news/MVA-202203.html"><strong aria-hidden="true">2.5.3.</strong> 3 月社区 MVA（Most Valuable Advocate）</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/community-news/contributors.html"><strong aria-hidden="true">2.5.4.</strong> Contributor 动态</a></li></ol></li><li class="chapter-item expanded "><a href="../../tidb-202203/tidb-certification/tidb-certification.html"><strong aria-hidden="true">2.6.</strong> TiDB 能力认证</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tidb-202203/tidb-certification/pcta-pctp.html"><strong aria-hidden="true">2.6.1.</strong> 认证介绍 &amp; 考试安排</a></li><li class="chapter-item expanded "><a href="../../tidb-202203/tidb-certification/tidb-course.html"><strong aria-hidden="true">2.6.2.</strong> TiDB 课程推荐</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">TiDB 社区技术月刊</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/luzizhuo/TiDB-Monthly-202203" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="tidb集群恢复之tikv集群不可用"><a class="header" href="#tidb集群恢复之tikv集群不可用">TiDB集群恢复之TiKV集群不可用</a></h1>
<p><strong>作者：代晓磊</strong></p>
<p>引入数学概率问题：之前上学时都学过，把几组小球放几个盒子，然后计算概率的问题，那么我有10组小球(每组3个)，放5个盒子里(每个盒子不能空着)，会有多大的概率在2个盒子损坏的情况下，保证每组小球至少保留1个？</p>
<p>Tikv可用性图 </p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/tikv-useable-1647483282645.png" alt="tikv-useable.png" /></p>
<p>下面对上图简单解释下：</p>
<p>（1）以上都是在region默认3副本的情况下的讨论。</p>
<p>(2) 以上的可用性都是在大量region的情况下，无论有多少个tikv节点，一个region的3个副本肯定会调度到KV集群的3个tikv上，3个副本的多数副本不可用，该region就不可用了，大量region的情况下，有很大的概率同一个raft group的2个peer正好调度在宕机的2个tikv上，这也是为啥文章开头说概率的意义。</p>
<p>（3）集群可用是指：就算DBA不介入，整个集群也会正常提供服务。比如拿5个节点的tikv cluster来讲，每次只能宕机1台，tikv宕机后，在该tikv上的leader region会根据raft-group来找follwer region来提升为新leader，并且等30分钟后其他tikv节点补副本，最终完成3个副本的raft-group；另外对于该tikv节点上的follwer region，在其他tikv节点的leader region也会在另外的tikv节点补follwer region节点(3个节点的tikv集群在宕机1台的情况下，没有多余的kv节点可以补副本，这时需要扩容tikv来补)。</p>
<p>（4）不丢数据是指：在3副本的情况下，如果多数副本(2个副本)不可用的情况下，但是还保留着一份副本（数据没有丢），SQL读写的表现就是：该region就不可用。</p>
<p>所以聊了3副本的可用性问题后，咱们就通过5个tikv节点的宕机测试来验证可用性以及数据恢复方案。</p>
<h2 id="实验模拟5节点集群宕机可用性"><a class="header" href="#实验模拟5节点集群宕机可用性">实验模拟5节点集群宕机可用性</a></h2>
<p>测试环境： </p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/tikv-down1-1647483335369.png" alt="tikv-down1.png" /></p>
<p>使用sysbench导入10张50万数据的table，然后sysbench读流量模拟请求。</p>
<pre><code>sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-host=10.xxxx.160 --mysql-port=4000 --mysql-db=test --mysql-user=root --mysql-password='xxx' --table_size=500000 --tables=10 --threads=30 --time=220 --report-interval=10 --db-driver=mysql  prepare
sysbench /usr/share/sysbench/oltp_read_only.lua --mysql-host=10.xxxx.160 --mysql-port=4000 --mysql-db=test --mysql-user=root --mysql-password='xxx' --table_size=500000 --tables=10 --threads=30 --time=2000 --report-interval=10 --db-driver=mysql run
</code></pre>
<h3 id="宕机1台"><a class="header" href="#宕机1台">宕机1台</a></h3>
<p>一般宕机1台不用太担心，因为如上所说，region的多数副本存活，该重新选leader的重新选，该补follwer副本的也会在其他节点补充。</p>
<p>在宕机1台，leader还没有选举成功或者follwer副本还没有补充完毕时，又宕机1台，其实跟下面要讲的同时宕机2台的问题分析和处理方式一样。</p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/tikv-sql-slow-1647483360496.png" alt="tikv-sql-slow.png" /></p>
<p>通过sysbench的压测任务可以看出，上图中红框位置出现20s左右的QPS抖动(QPS由之前的平均1.2万降低到4千左右)，因为SQL正在访问的leader region节点发生故障，导致raft重新选举新leader后恢复正常，下面是tiup pd ctl命令来查看宕机tikv的store信息。</p>
<pre><code>tiup ctl:v5.1.1 pd -u http://10.xxxxx.173:2379 store|grep -B 10 'Disconnected'
Starting component `ctl`: /home/tidb/.tiup/components/ctl/v5.1.1/ctl /home/tidb/.tiup/components/ctl/v5.1.1/ctl pd -u http://10.xxxxx.173:2379 store
    {
      &quot;store&quot;: {
        &quot;id&quot;: 5,
        &quot;address&quot;: &quot;10.xxxx.155:20160&quot;,
        &quot;version&quot;: &quot;5.1.1&quot;,
        &quot;status_address&quot;: &quot;10.xxxx.155:20180&quot;,
        &quot;git_hash&quot;: &quot;4705d7c6e9c42d129d3309e05911ec6b08a25a38&quot;,
        &quot;start_timestamp&quot;: 1628479214,
        &quot;deploy_path&quot;: &quot;/data6/deploy/tikv-20180/bin&quot;,
        &quot;last_heartbeat&quot;: 1646640368489894705,
        &quot;state_name&quot;: &quot;Disconnected&quot;
</code></pre>
<p>通过以上可以看到tikv状态为disconnected，在这里简单提下tikv的状态，正常启动就是UP状态，当tikv节点跟PD断开超过20s后转变为Disconnected状态，默认超过30分钟(max-store-down-time设定)后tikv转变为down状态，<strong>只有变为down状态，其他存活的tikv才会补该tikv节点的region副本</strong>。对于down的tikv我们scale-in后出现offline状态，一旦所有的region都正常后最终变为tombstone状态。</p>
<h2 id="同时宕机2台"><a class="header" href="#同时宕机2台">同时宕机2台</a></h2>
<p>我们下面通过 rm -rf tikv-data 来暴力模拟故障，同时删除 2 个 tikv 的 data 来模拟5个tikv集群2个tikv宕机处理，看下集群当前情况： </p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/2tikv-down-1647483380541.png" alt="2tikv-down.png" /></p>
<p>再看下sysbench压测请求的QPS情况，从打印的日志明显看到有 tikv server timeout 和 region is unavailable 的报错,sysbench报错后续的QPS已经变0。</p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/tikv_rw_error-1647483405780.png" alt="tikv_rw_error.png" /></p>
<p>下面开始进行tikv集群的恢复，操作步骤如下：</p>
<p>（1）使用tiup ctl pd(同之前的pd-ctl命令)来查看下 Tikv 哪些store id不可用了，发现是store 1和6。</p>
<pre><code>tiup ctl:v5.1.1 pd -u http://xxxx:2379 store|grep -B 10 'Disconnected'
    {
      &quot;store&quot;: {
        &quot;id&quot;: 6,
        &quot;address&quot;: &quot;10.xxxx.201:20160&quot;,
        &quot;version&quot;: &quot;5.1.1&quot;,
        &quot;state_name&quot;: &quot;Disconnected&quot;
--
    {
      &quot;store&quot;: {
        &quot;id&quot;: 1,
        &quot;address&quot;: &quot;10.xxxx.218:20160&quot;,
        &quot;version&quot;: &quot;5.1.1&quot;,
        &quot;state_name&quot;: &quot;Disconnected&quot;
</code></pre>
<p>（2）PD调度关闭，避免恢复过程中产生的各种异常情况，对了，在将下面的参数调整为0之前，建议先tiup ctl pd config show看下之前的参数值为多少，另外使用tiup ctl时需要选择跟tidb集群版本一致的ctl version。</p>
<pre><code>$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set region-schedule-limit 0
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set replica-schedule-limit 0
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set leader-schedule-limit 0
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set merge-schedule-limit 0
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 operator show
[]
</code></pre>
<p>通过上面的命令可以看到调度已经关闭。</p>
<p>（3）停止其他UP状态的tikv,目的避免新的写入导致的region副本之间的元信息不一致，另外就是释放文件锁。</p>
<pre><code>tiup cluster stop BA-xxxx_bak -R tikv
</code></pre>
<p>如果上面命令停不掉，可以登录到tikv节点使用systemctl stop tikv-20160停掉tikv</p>
<p>（4）在刚才关闭的每个tikv节点，使用tikv-ctl强制region从多副本失效的状态恢复，unsafe-recover remove-fail-stores 命令可以将故障机器从指定 Region 的 peer 列表中移除。运行命令之前，需要目标 TiKV 先停掉服务以便释放文件锁（否则执行tikv-ctl恢复时有下面的报错）。</p>
<pre><code>[2022/03/14 17:51:54.215 +08:00] [ERROR] [main.rs:78] [&quot;error while open kvdb: Storage Engine IO error: While lock file: /data6/tikv-20180/db/LOCK: Resource temporarily unavailable&quot;]
[2022/03/14 17:51:54.215 +08:00] [ERROR] [main.rs:81] [&quot;LOCK file conflict indicates TiKV process is running. Do NOT delete the LOCK file and force the command to run. Doing so could cause data corruption.&quot;]
</code></pre>
<p>tikv-ctl的-s 选项是指宕机的多个以逗号分隔的 store_id (本次实验为1和6)，可以使用 -r 后面跟多个逗号分隔的Region id来指定要移除的peer，如果集群过大，需要移除的region id太多了，可简单指定 --all-regions 来对存活 store 上的全部 Region 都执行这个操作。</p>
<pre><code>将跟集群版本适配的tikv-ctl拷贝到存活tikv节点
scp /home/tidb/.tiup/components/ctl/v5.1.1/tikv-ctl tidb@10.xxxx.155:/home/tidb
scp /home/tidb/.tiup/components/ctl/v5.1.1/tikv-ctl tidb@10.xxxx.208:/home/tidb
scp /home/tidb/.tiup/components/ctl/v5.1.1/tikv-ctl tidb@10.xxxx.238:/home/tidb
在每个存活tikv节点都执行下面的tikv-ctl命令(注意要在tikv stop的情况下)
$ ./tikv-ctl --data-dir /data6/tikv-20180 unsafe-recover remove-fail-stores -s 6,1 --all-regions
[2022/03/10 16:20:40.987 +08:00] [INFO] [mod.rs:118] [&quot;encryption: none of key dictionary and file dictionary are found.&quot;]
[2022/03/10 16:20:40.987 +08:00] [INFO] [mod.rs:479] [&quot;encryption is disabled.&quot;]
[2022/03/10 16:20:41.032 +08:00] [WARN] [config.rs:581] [&quot;compaction guard is disabled due to region info provider not available&quot;]
[2022/03/10 16:20:41.032 +08:00] [WARN] [config.rs:675] [&quot;compaction guard is disabled due to region info provider not available&quot;]
removing stores [6, 1] from configurations...
[2022/03/10 16:20:41.236 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 1722 store_id: 5]&quot;] [old_peers=&quot;[id: 1722 store_id: 5, id: 2111 store_id: 6, id: 2117 store_id: 1]&quot;] [region_id=18]
[2022/03/10 16:20:41.236 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 1982 store_id: 5]&quot;] [old_peers=&quot;[id: 1944 store_id: 1, id: 1982 store_id: 5, id: 2119 store_id: 6]&quot;] [region_id=26]
[2022/03/10 16:20:41.236 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 1858 store_id: 5]&quot;] [old_peers=&quot;[id: 1858 store_id: 5, id: 2108 store_id: 6, id: 2113 store_id: 1]&quot;] [region_id=38]
[2022/03/10 16:20:41.236 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 1717 store_id: 5]&quot;] [old_peers=&quot;[id: 1717 store_id: 5, id: 2110 store_id: 6, id: 2115 store_id: 1]&quot;] [region_id=46]
[2022/03/10 16:20:41.236 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 1859 store_id: 5]&quot;] [old_peers=&quot;[id: 1859 store_id: 5, id: 2112 store_id: 6, id: 2114 store_id: 1]&quot;] [region_id=1009]
.....此处省略N行
[2022/03/10 16:20:41.237 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 1880 store_id: 7, id: 1881 store_id: 5]&quot;] [old_peers=&quot;[id: 1880 store_id: 7, id: 1881 store_id: 5, id: 1887 store_id: 1]&quot;] [region_id=1879]
[2022/03/10 16:20:41.237 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 1884 store_id: 7, id: 1886 store_id: 5, id: 2107 store_id: 4]&quot;] [old_peers=&quot;[id: 1884 store_id: 7, id: 1886 store_id: 5, id: 2107 store_id: 4]&quot;] [region_id=1883]
success
</code></pre>
<p>注意：--all-regions是需要在所有store节点上执行的。另外就是使用了remove-fail-store参数后，已经被移除的节点(故障tikv节点)一定不能再加入集群，否则会导致PD元信息不一致。</p>
<p>(5)恢复pd调度配置</p>
<pre><code>$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set region-schedule-limit 2048
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set replica-schedule-limit 64
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set leader-schedule-limit 4
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 config set merge-schedule-limit 8
</code></pre>
<p>(6) 启动tikv集群，我这里是通过-R指定所有tikv重启，其实建议-N 指定之前状态正常的3个tikv节点启动。</p>
<pre><code>tiup cluster start BA-analyse-tidb_shyc_bak -R tikv
</code></pre>
<p>启动过程出现报错，因为无论是我模拟的tikv data目录被删，亦或是硬盘故障，主机宕机等，还是之前的store id为1和6的tikv启动失败，通过tiup cluster display 查看还是有3个tikv节点启动成功的，不影响业务正常使用，通过继续跑sysbench和count全表，没有出现读写和表数据丢失问题。下图是随便找了2张table查看数据量也是50w，跟之前sysbench导入的数据量一致。</p>
<pre><code>mysql&gt; select count(1) from sbtest10;
+----------+
| count(1) |
+----------+
|   500000 |
+----------+
1 row in set (0.41 sec)

mysql&gt; select count(1) from sbtest3;
+----------+
| count(1) |
+----------+
|   500000 |
+----------+
1 row in set (0.41 sec)
</code></pre>
<p>集群现状： </p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/tikv-ok-1647483426104.png" alt="tikv-ok.png" /></p>
<p>（7）后续处理：本次的事故模拟是通过rm -rf tikv-data目录来实现的，并且没有从PD的store信息里删除已经down的store id：1和6，所以上面的tikv重启操作默认会在“误操作”的tikv节点重新创建目录和启动tikv，但是启动不成功，提示duplicated store address，“新的tikv”跟老的tikv都是同一个ip和端口，自然启动不成功，报错如下：</p>
<pre><code>[2022/03/10 17:00:21.320 +08:00] [ERROR] [util.rs:460] [&quot;request failed&quot;] [err_code=KV:PD:gRPC] [err=&quot;Grpc(RpcFailure(RpcStatus { code: 2-UNKNOWN, message: \&quot;duplicated store address: id:1194100 address:\\\&quot;10.218.93.201:20160\\\&quot; version:\\\&quot;5.1.1\\\&quot; status_address:\\\&quot;10.218.93.201:20180\\\&quot; git_hash:\\\&quot;4705d7c6e9c42d129d3309e05911ec6b08a25a38\\\&quot; start_timestamp:1646902817 deploy_path:\\\&quot;/data6/deploy/tikv-20180/bin\\\&quot; , already registered by id:6 address:\\\&quot;10.218.93.201:20160\\\&quot; version:\\\&quot;5.1.1\\\&quot; status_address:\\\&quot;10.218.93.201:20180\\\&quot; git_hash:\\\&quot;4705d7c6e9c42d129d3309e05911ec6b08a25a38\\\&quot; start_timestamp:1628479259 deploy_path:\\\&quot;/data6/deploy/tikv-20180/bin\\\&quot; last_heartbeat:1646897290357034075 \&quot;, details: [] }))
</code></pre>
<p>解决方案：pd元信息删除原来的store id，然后再重启下2个被rm又重新加入集群的“新节点”</p>
<pre><code>$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 store delete 1
$ tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 store delete 6
$ tiup cluster start BA-analyse-tidb_shyc_bak -N 10.xxxx.218:20160
$ tiup cluster start BA-analyse-tidb_shyc_bak -N 10.xxxx.201:20160
</code></pre>
<p>到目前，整个集群又恢复到5个tikv完全可用的情况了，本次模拟了5个tikv节点中2个tikv数据被删除的情况下，如何恢复数据的方式和方法。</p>
<h3 id="宕机3台"><a class="header" href="#宕机3台">宕机3台</a></h3>
<p>这次玩大的，5台机器，直接shutdown 3个tikv节点的服务器，模拟硬件故障启动不起来。 </p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/3tikv_down-1647483441832.png" alt="3tikv_down.png" /></p>
<p>处理步骤还是跟上面2KV宕机的步骤类似(但还是有不同)</p>
<p>（1）pd ctl查看宕机的store id，发现是1，2，8</p>
<pre><code>tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 store|grep -B 10 'Disconnected'
</code></pre>
<p>（2）PD调度关闭，避免恢复过程中产生的各种异常情况。</p>
<p>（3）检查大于等于一半副本数在故障节点上的region</p>
<pre><code>tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 region --jq='.regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(length as $total | map(if .==(2,8,1) then . else empty end) | length&gt;=$total-length)}'
{&quot;id&quot;:199,&quot;peer_stores&quot;:[8,2,5]}
{&quot;id&quot;:677,&quot;peer_stores&quot;:[11,1,8]}
{&quot;id&quot;:736,&quot;peer_stores&quot;:[2,1,5]}
{&quot;id&quot;:14,&quot;peer_stores&quot;:[8,2,1]}
{&quot;id&quot;:48,&quot;peer_stores&quot;:[1,2,5]}
{&quot;id&quot;:52,&quot;peer_stores&quot;:[8,2,1]}
{&quot;id&quot;:756,&quot;peer_stores&quot;:[1,8,11]}
{&quot;id&quot;:46,&quot;peer_stores&quot;:[8,2,1]}
  .....此处省略N行
{&quot;id&quot;:175,&quot;peer_stores&quot;:[2,1,8]}
{&quot;id&quot;:203,&quot;peer_stores&quot;:[1,8,2]}
{&quot;id&quot;:211,&quot;peer_stores&quot;:[8,5,2]}
{&quot;id&quot;:22,&quot;peer_stores&quot;:[2,1,11]}
{&quot;id&quot;:36,&quot;peer_stores&quot;:[8,2,5]}
{&quot;id&quot;:730,&quot;peer_stores&quot;:[5,8,1]}
{&quot;id&quot;:611,&quot;peer_stores&quot;:[8,11,1]}
{&quot;id&quot;:58,&quot;peer_stores&quot;:[8,2,1]}
{&quot;id&quot;:131,&quot;peer_stores&quot;:[2,1,11]}
{&quot;id&quot;:18,&quot;peer_stores&quot;:[8,5,2]}
{&quot;id&quot;:30,&quot;peer_stores&quot;:[2,1,11]}
</code></pre>
<p>大家可以看到region id 为 52、46、175、58的3个副本都丢失了(leader和2个follower都在宕机的3个tikv节点，如果3个tikv都无法启动或者恢复，那这4个region的数据就丢了)。另外还有不少只剩下1个peer的region id，比如199、677、736等只丢了2个副本(还剩1个peer，数据可以找回)。</p>
<p>（4）使用tiup cluster stop -N 来关闭存活的2个tikv</p>
<p>(5) 在所有未宕机的tikv节点,对所有region移除故障节点的peer。</p>
<pre><code>./tikv-ctl --data-dir /data6/tikv-20180/ unsafe-recover remove-fail-stores -s 2,8,1 --all-regions
[2022/03/14 20:16:22.059 +08:00] [INFO] [mod.rs:118] [&quot;encryption: none of key dictionary and file dictionary are found.&quot;]
[2022/03/14 20:16:22.060 +08:00] [INFO] [mod.rs:479] [&quot;encryption is disabled.&quot;]
[2022/03/14 20:16:22.104 +08:00] [WARN] [config.rs:581] [&quot;compaction guard is disabled due to region info provider not available&quot;]
[2022/03/14 20:16:22.104 +08:00] [WARN] [config.rs:675] [&quot;compaction guard is disabled due to region info provider not available&quot;]
removing stores [2, 8, 1] from configurations...
[2022/03/14 20:16:25.909 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 298 store_id: 5]&quot;] [old_peers=&quot;[id: 298 store_id: 5, id: 460 store_id: 8, id: 838 store_id: 2]&quot;] [region_id=3]
[2022/03/14 20:16:25.909 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 795 store_id: 5]&quot;] [old_peers=&quot;[id: 13 store_id: 1, id: 755 store_id: 2, id: 795 store_id: 5]&quot;] [region_id=12]
[2022/03/14 20:16:25.909 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 829 store_id: 5]&quot;] [old_peers=&quot;[id: 83 store_id: 8, id: 686 store_id: 2, id: 829 store_id: 5]&quot;] [region_id=16]
[2022/03/14 20:16:25.909 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 789 store_id: 5, id: 940 store_id: 11 role: Learner]&quot;] [old_peers=&quot;[id: 715 store_id: 8, id: 789 store_id: 5, id: 836 store_id: 2, id: 940 store_id: 11 role: Learner]&quot;] [region_id=18]
 ........此处省略N行
[2022/03/14 20:16:25.910 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 447 store_id: 11, id: 688 store_id: 5]&quot;] [old_peers=&quot;[id: 445 store_id: 2, id: 447 store_id: 11, id: 688 store_id: 5]&quot;] [region_id=444]
[2022/03/14 20:16:25.910 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 633 store_id: 5]&quot;] [old_peers=&quot;[id: 633 store_id: 5, id: 778 store_id: 1, id: 827 store_id: 2]&quot;] [region_id=632]
[2022/03/14 20:16:25.910 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 731 store_id: 5]&quot;] [old_peers=&quot;[id: 731 store_id: 5, id: 732 store_id: 8, id: 825 store_id: 1]&quot;] [region_id=730]
[2022/03/14 20:16:25.910 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 818 store_id: 5]&quot;] [old_peers=&quot;[id: 737 store_id: 2, id: 745 store_id: 1, id: 818 store_id: 5]&quot;] [region_id=736]
[2022/03/14 20:16:25.910 +08:00] [INFO] [debug.rs:586] [&quot;peers changed&quot;] [new_peers=&quot;[id: 750 store_id: 11, id: 751 store_id: 5]&quot;] [old_peers=&quot;[id: 750 store_id: 11, id: 751 store_id: 5, id: 773 store_id: 8]&quot;] [region_id=749]
success
</code></pre>
<p>注意：因为是5个KV宕机3台，需要在stop tikv server的剩余2台tikv都执行。</p>
<p>(6)重启2个存活的tikv</p>
<p>(7)再次查看多数副本在宕机的3台tikv上的region，发现就剩下这些3个副本都丢失的region了</p>
<pre><code>tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 region --jq='.regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(length as $total | map(if .==(2,8,1) then . else empty end) | length&gt;=$total-length)}'
Starting component `ctl`: /home/tidb/.tiup/components/ctl/v5.1.1/ctl /home/tidb/.tiup/components/ctl/v5.1.1/ctl pd -u http://10.xxxxx:2379 region --jq=.regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(length as $total | map(if .==(2,8,1) then . else empty end) | length&gt;=$total-length)}
{&quot;id&quot;:52,&quot;peer_stores&quot;:[8,2,1]}
{&quot;id&quot;:46,&quot;peer_stores&quot;:[8,2,1]}
{&quot;id&quot;:28,&quot;peer_stores&quot;:[8,2,1]}
{&quot;id&quot;:203,&quot;peer_stores&quot;:[1,8,2]}
{&quot;id&quot;:175,&quot;peer_stores&quot;:[2,1,8]}
{&quot;id&quot;:58,&quot;peer_stores&quot;:[8,2,1]}
{&quot;id&quot;:14,&quot;peer_stores&quot;:[8,2,1]}
</code></pre>
<p>（8）查看这些region id所属的table，下面拿203这个region id来查看，发现是属于test库的sbtest5表，需要查看下所有3个副本都丢失的region都属于哪些表，有时候涉及的region过多，可以搞个小脚本批量执行，执行结果都汇总到一个文本中，以便后续补数和业务沟通，这种情况是：反正数据已经丢了，避免业务访问到这些reigon的报错，需要跟业务沟通是否用空region代替，后续通过业务或者其他方式找回。</p>
<pre><code> curl http://10.xxxx.160:10086/regions/203
{
 &quot;start_key&quot;: &quot;dIAAAAAAAAA9X3KAAAAAAARH7A==&quot;,
 &quot;end_key&quot;: &quot;dIAAAAAAAABF&quot;,
 &quot;start_key_hex&quot;: &quot;74800000000000003d5f7280000000000447ec&quot;,
 &quot;end_key_hex&quot;: &quot;748000000000000045&quot;,
 &quot;region_id&quot;: 203,
 &quot;frames&quot;: [
  {
   &quot;db_name&quot;: &quot;test&quot;,
   &quot;table_name&quot;: &quot;sbtest5&quot;,
   &quot;table_id&quot;: 61,
   &quot;is_record&quot;: true,
   &quot;record_id&quot;: 280556
  }
 ]
}
</code></pre>
<p>(9)再次关停存活的2个tikv并且在这2个tikv上补这种3个副本都丢了的region，用空region补充这些peer副本。</p>
<pre><code>   ./tikv-ctl --data-dir /data6/tikv-20180/ recreate-region -p 10.xxxxx:2379 -r 52
   ./tikv-ctl --data-dir /data6/tikv-20180/ recreate-region -p 10.xxxxx:2379 -r 46
</code></pre>
<p>注意：补空region时 -r 命令后不能带多个逗号分隔的region id，只能一个region一个region的补，如果region id过多，可以考虑搞个脚本来跑。</p>
<p>(10)恢复pd调度配置</p>
<p>（11）重启2个tikv，再次查看region情况，发现所有的region的多数副本都已经正常。</p>
<p>tiup ctl:v5.1.1 pd -u http://10.xxxxx:2379 region --jq='.regions[] | {id: .id, peer<em>stores: [.peers[].store</em>id] | select(length as $total | map(if .==(2,8,1) then . else empty end) | length&gt;=$total-length)}'</p>
<p>(12) 登录查看数据，通过上面的第5步骤的region丢失来看查看，统计发现sbtest5/sbtest6表丢失了近1半的记录，其他数据表有完整数据的。</p>
<pre><code>mysql&gt; select count(1),count(c) from sbtest5;
+----------+----------+
| count(1) | count(c) |
+----------+----------+
|   280555 |   280555 |
+----------+----------+
1 row in set (0.35 sec)

mysql&gt; select count(1),count(c) from sbtest6;
+----------+----------+
| count(1) | count(c) |
+----------+----------+
|   247312 |   247312 |
+----------+----------+
1 row in set (0.27 sec)

mysql&gt; select count(1),count(c) from sbtest1;
+----------+----------+
| count(1) | count(c) |
+----------+----------+
|   500000 |   500000 |
+----------+----------+
1 row in set (0.38 sec)
</code></pre>
<p>目前集群状态： </p>
<p><img src="https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/3tikv-ok-1647483461131.png" alt="3tikv-ok.png" /></p>
<h2 id="总结和思考"><a class="header" href="#总结和思考">总结和思考</a></h2>
<h3 id="总结"><a class="header" href="#总结">总结</a></h3>
<p>本篇文章主要讲了tikv节点宕机时的现象以及如何恢复的方案，tikv 3副本的 raft group能保证在宕机一台KV时不用DBA立刻介入的能力；在上面宕机 2 台 KV 时，因为根据PD的调度规则，一个raft group的peer肯定会调度到3个KV节点，所以2个KV宕机，肯定还有一个peer在，需要DBA介入，并且使用“快刀斩乱麻”的tikv-ctl尽快的恢复了集群；但是在集群宕机 3 台 KV 时就有大概率的数据丢失，这时候就需要考虑业务恢复重要还是数据保护重要了，另外在3个kv宕机的章节，我还重启了2次tikv，其实如果定好了预案，清理region中故障store的peer跟 recreate 空region可以一起做，本次只是突出region丢失数据的严重性。</p>
<p>另外以上是基于5个KV的集群做的多次验证，每次都出了不少的“状况”，可以根据文章的大概处理流程自己模拟验证，其实我遇到一个“严重”的“状况”是：5副本的KV当挂了3个时，可能一些mysql系统表(比如user权限表)的region也“丢失”了，导致无法进入到集群验证数据，后来通过：跟mysql类似的skip-grant-table才进入，如果用户权限都没有了，业务肯定都无法连接tidb了，这种情景就是要强调：在我没有BR备份的情况下，5个节点(3kv宕机)能给我恢复出2个kv数据的重要性了。</p>
<p>还有一个就是补完空region，还遇到了一些诡异的“数据准确性”问题，比如sbtest5表的默认索引k<em>5索引被补了空region时，idx</em>k是我重新建立的索引。通过2个索引的查询结果是不一样的，也就是说补region影响了数据的正确性，不过对于上面的集群多数KV不可用的情况下，还是建议新建集群然后将2个KV的数据恢复到新集群。</p>
<pre><code>mysql&gt; show create table sbtest5\G
*************************** 1. row ***************************
       Table: sbtest5
Create Table: CREATE TABLE `sbtest5` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `k` int(11) NOT NULL DEFAULT '0',
  `c` char(120) NOT NULL DEFAULT '',
  `pad` char(60) NOT NULL DEFAULT '',
  PRIMARY KEY (`id`) /*T![clustered_index] CLUSTERED */,
  KEY `k_5` (`k`),
  KEY `idx_k` (`k`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin AUTO_INCREMENT=633209
1 row in set (0.00 sec)

mysql&gt; select * from sbtest5 use index(k_5) where k=249602;
ERROR 1105 (HY000): inconsistent index k_5 handle count 99 isn't equal to value count 66
mysql&gt;
mysql&gt; select count(1) from sbtest5 use index(idx_k) where k=249602;
+----------+
| count(1) |
+----------+
|       66 |
+----------+
1 row in set (0.00 sec)

mysql&gt; select count(1) from sbtest5 use index(idx_k);
+----------+
| count(1) |
+----------+
|   280555 |
+----------+
1 row in set (0.20 sec)

mysql&gt; select count(1) from sbtest5 use index(k_5);
+----------+
| count(1) |
+----------+
|   500000 |
+----------+
1 row in set (0.27 sec)
</code></pre>
<h3 id="思考"><a class="header" href="#思考">思考：</a></h3>
<p>（1） 在集群多tikv(&gt;=3)不可用的时候，需要根据业务情况做好降级准备，最好在sql接入层（HAproxy/lvs）先把vip下线，避免新流量写入，毕竟后面tikv-ctl操作涉及到tikv所有节点的shutdown。</p>
<p>（2）以上的测试都是基于默认的 3 副本来分析的，对于多 KV 的核心集群，其实可以设定 5 副本来增加集群的可用性。</p>
<p>（3）关闭PD调度和避免宕机tikv重启的重要性，避免元信息异常引发的region不可用</p>
<p>（4）tikv真的不可以恢复后的补region的决定，需要根据实际情况来定，另外一定要看下补的region都是哪些业务的什么table or index，注意观察下“修复”后数据的准确性。</p>
<p>（5）做好TiDB的备份，如果真的线上集群不可用，至少还有备份可以恢复，就看备份的频度（容忍丢失的数据量），一旦集群多数KV不可用，需要”2条腿“走路，1个方案是BR恢复备份到新集群，另外就是在原有集群进行tikv-ctl的恢复。</p>
<p>（6）“多活”方案的重要性，比如基于ticdc的主备同城双机房，备机房可以做一些准实时的读请求(读写分离)；另外就是 tidb 5.4 版本提供的同城双中心自适应同步模式；其他同城3中心或者异地3中心的方案可以参考官方文档。</p>
<p>（7）同一个集群同时3个tikv宕机的概率还是比较低的，除非就是部署时没有考虑机架或者交换机部署，由于机架掉电或者交换机故障导致的多KV的不可用，这是就要看业务是否能扛或者IDC换件的应急速度了。</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../tidb-202203/usercase/2-inspur-tidb.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="../../tidb-202203/usercase/4-PointGet.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../tidb-202203/usercase/2-inspur-tidb.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="../../tidb-202203/usercase/4-PointGet.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
    </body>
</html>
